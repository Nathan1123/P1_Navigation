<h1 id="project-1-navigation">Project 1: Navigation</h1>
<p>Nathan Goedeke</p>
<p>Deep Reinforcement Learning</p>
<p>March 28, 2022</p>
<h3 id="algorithm-summary">Algorithm Summary</h3>
<p>The approach to this project uses the Deep Q-Network Reinforcement Learning Algorithm. Reinforcement Learning creates a table of values Q(S,A) that estimates the expected payout by taking a given action A while in state S. With each new state, the agent selects the action giving the largest expected payout from the Q-table. The function giving best action for each state is then called the policy. After recieving a new state and reward from this action, the agent updates the Q-table using the information (S,A,R,S&#39;,A&#39;). Thus, the reinforcement algorithm alternates in a cycle of three steps:</p>
<ol>
<li><p>Create a new policy Pi(S) from the Q-table by selecting the best action A for each state.</p>
</li>
<li><p>Select new action A&#39; from the policy, and obtain reward R and new state S&#39;</p>
</li>
<li><p>Update the Q-table at entry Q(A,S) using (S,A,R,S&#39;,A&#39;)</p>
</li>
</ol>
<p>Creating a policy using the maximum expected payout is a greedy approach. Alternatively, a probability factor epsilon is used to balance between exploitation and exploration, such that a value of eps=0 is a greedy approach while eps=1 selects actions completley at random. </p>
<p>A Deep Q-Network augments this above approach by using a neural network to approximate the function Q(S,A). The information of (S,A,R,S&#39;,A&#39;) is used to update the weights of this network at a given rate. </p>
<p>An experience replay buffer is used to record previous tuples of data (S,A,R,S&#39;,A&#39;), which is pulled from randomly in order to update the network from previous runs. </p>
<h3 id="parameters">Parameters</h3>
<p>These are all the parameters used to generate this model. These first four parameters are already given by the nature of the problem:</p>
<ul>
<li>State Size - The number of dimensions in the environment (37)</li>
<li>Action Size - The number of possible actions at any given state (4)</li>
<li>Seed - Pseudo- Random seed used in the learning model. Set to zero here for true randomness</li>
<li>Score Window - Number of episodes to average for a total score (100)</li>
</ul>
<p>These remaining parameters are determined by experimentation and research:</p>
<ul>
<li>Buffer size - Number of tuples (S,A,R,S&#39;,A&#39;) stored in the experience replay buffer</li>
<li>Batch size - Size of batch data in the neural network</li>
<li>Gamma - The discount factor, which determines how much information diminishes over multiple runs. A value of 1 remembers everything and a value of 0 remembers nothing.</li>
<li>Tau - Controls how much information is balanced between the Target Q-Network and the Local Q-Network</li>
<li>Learning Rate - How sensitive the neural network is to updating weights</li>
<li>Update Every - How many actions are taken before updating the network weights</li>
<li>N Episodes - Total number of episodes to run</li>
<li>Max T - Maximum number of actions to take before ending an episode. Otherwise, the episode will end when the environment returns Done=True</li>
<li>Epsilon - Value to balance exploration and exploitation, where a value of zero will be a greedy approach. In this project, epsilon starts at a value of one and diminishes at a rate of EPS_Decay, stopping at a minimum value EPS_End. </li>
</ul>
<h3 id="results">Results</h3>
<p>The agent successfully trained after 1800 episodes to collect an average reward of 15-16 over 100 episodes. The results per 100 episodes and chart are displayed below:</p>
<p>Episode 100     Average Score: 0.54<br>
Episode 200     Average Score: 4.03<br>
Episode 300     Average Score: 7.45<br>
Episode 400     Average Score: 9.42<br>
Episode 500     Average Score: 12.40<br>
Episode 600     Average Score: 13.41<br>
Episode 700     Average Score: 14.43<br>
Episode 800     Average Score: 15.08<br>
Episode 900     Average Score: 15.54<br>
Episode 1000     Average Score: 14.54<br>
Episode 1100     Average Score: 15.77<br>
Episode 1200     Average Score: 14.88<br>
Episode 1300     Average Score: 15.42<br>
Episode 1400     Average Score: 15.84<br>
Episode 1500     Average Score: 15.36<br>
Episode 1600     Average Score: 15.53<br>
Episode 1700     Average Score: 15.67<br>
Episode 1800     Average Score: 14.73</p>
<p><img src="https://i.imgur.com/SQxhG23.png" alt="Results Chart" title="Results Chart"></p>
<h3 id="future-work">Future Work</h3>
<ul>
<li>Better performance could possibly be achieved by fine tuning other parameters. It seems likely the agent could get as high as 18</li>
<li>Implementing a priority factor to the experience replay buffer could make better use of the replay system</li>
</ul>
